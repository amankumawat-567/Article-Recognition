{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOyFwvQBS6F/4UK17Qnj0Eb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTv2xCaFllJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRT Model"
      ],
      "metadata": {
        "id": "_snHfrKzzwlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SELF-ATTENTION Calculation"
      ],
      "metadata": {
        "id": "q68nbZ-oGAsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth) # NOTE: depth=dk\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk. underroot d_model i.e. underroot(100)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "AjAZ9EhGFwO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttention Calculation"
      ],
      "metadata": {
        "id": "h9p45La2GGAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model  # typically 512\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "QgJtGON8GLFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layers"
      ],
      "metadata": {
        "id": "c75ksvR8G-mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, rate=0.1):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x1, x2, training):\n",
        "        attn1_output, _ = self.mha1(x1, x1, x1)\n",
        "        attn1_output = self.dropout1(attn1_output, training=training)\n",
        "        outx1 = self.layernorm1(x1 + attn1_output)\n",
        "\n",
        "        attn2_output, _ = self.mha2(x2, x2, x2)\n",
        "        attn2_output = self.dropout2(attn2_output, training=training)\n",
        "        outx2 = self.layernorm2(x2 + attn2_output)\n",
        "\n",
        "        attn3_output, _ = self.cross_mha(outx1, outx1, outx2)\n",
        "        attn3_output = self.dropout3(attn3_output, training=training)\n",
        "        out = self.layernorm3(outx2 + attn3_output)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "olBt_yK5HIM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLevelAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, input_vocab_size, rate=0.1):\n",
        "        super(MultiLevelAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding1 = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.embedding2 = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "        self.attention_layers = AttentionLayer(d_model, num_heads, rate)\n",
        "\n",
        "    def call(self, x1, x2, training):\n",
        "\n",
        "        x1 = self.embedding1(x1)\n",
        "        x2 = self.embedding2(x2)\n",
        "\n",
        "        x = self.attention_layers(x1, x2, training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "4N1EkwAsPX-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual Feed Forward Layer"
      ],
      "metadata": {
        "id": "2sfe0zFTUK2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "class DualFeedForwardLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, rate=0.1):\n",
        "        super(DualFeedForwardLayer, self).__init__()\n",
        "\n",
        "        self.ffn1 = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.ffn2 = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        ffn1_output = self.ffn1(x)\n",
        "        ffn1_output = self.dropout1(ffn1_output, training=training)\n",
        "        out1 = self.layernorm1(x + ffn1_output)\n",
        "\n",
        "        ffn2_output = self.ffn2(x)\n",
        "        ffn2_output = self.dropout2(ffn2_output, training=training)\n",
        "        out2 = self.layernorm2(x + ffn2_output)\n",
        "\n",
        "        return out1, out2"
      ],
      "metadata": {
        "id": "6iSCQL2lUSl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Regression Transformer"
      ],
      "metadata": {
        "id": "mXPu2LcKRq9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_categories, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.mla = MultiLevelAttention(d_model, num_heads, input_vocab_size, rate)\n",
        "\n",
        "        self.dffnl = DualFeedForwardLayer(d_model, dff, rate)\n",
        "\n",
        "        self.Classification_layer = tf.keras.layers.Dense(target_categories)\n",
        "        self.Regression_Layer = tf.keras.layers.Dense(1, activation='relu')\n",
        "\n",
        "    def call(self, x1, x2, training):\n",
        "\n",
        "        mla_output = self.mla(x1, x2, training)\n",
        "\n",
        "        dffnl_output1, dffnl_output2 = self.dffnl(mla_output, training)\n",
        "\n",
        "        final_output1 = self.Classification_layer(tf.reduce_mean(dffnl_output1, axis=1))\n",
        "        final_output2 = self.Regression_Layer(tf.reduce_mean(dffnl_output2, axis=1))\n",
        "\n",
        "        final_output1 = tf.nn.softmax(final_output1, axis=-1)\n",
        "\n",
        "        return final_output1, final_output2"
      ],
      "metadata": {
        "id": "dc0LDI-3RrPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summery"
      ],
      "metadata": {
        "id": "EbR6d3_Owvxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Transformer model hyperparameters\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "dff = 128\n",
        "input_vocab_size = 10000\n",
        "target_categories = 1200\n",
        "rate = 0.1\n",
        "\n",
        "# Assuming you have instantiated your model\n",
        "sample_transformer = Transformer(d_model, num_heads, dff, input_vocab_size, target_categories)\n",
        "\n",
        "# Temp Imputs\n",
        "batch_size = 32\n",
        "sequence_length1 = 100\n",
        "sequence_length2 = 150\n",
        "dummy_input1 = tf.ones((batch_size, sequence_length1))\n",
        "dummy_input2 = tf.ones((batch_size, sequence_length2))\n",
        "\n",
        "# Call the model on the dummy data to build the model\n",
        "sample_transformer(dummy_input1, dummy_input2)\n",
        "\n",
        "# Now you can print the summary\n",
        "sample_transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pAw3yVxM67O",
        "outputId": "e5f38089-6ead-4d81-846c-7ee415021c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " multi_level_attention (Mul  multiple                  2758912   \n",
            " tiLevelAttention)                                               \n",
            "                                                                 \n",
            " dual_feed_forward_layer (D  multiple                  66560     \n",
            " ualFeedForwardLayer)                                            \n",
            "                                                                 \n",
            " dense_16 (Dense)            multiple                  154800    \n",
            "                                                                 \n",
            " dense_17 (Dense)            multiple                  129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2980401 (11.37 MB)\n",
            "Trainable params: 2980401 (11.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the Transformer model"
      ],
      "metadata": {
        "id": "Jr_aPK3Bx-f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = Transformer(d_model, num_heads, dff, input_vocab_size, target_categories)"
      ],
      "metadata": {
        "id": "N_GeBQo1x-Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the Loss and Accuracy Matrics"
      ],
      "metadata": {
        "id": "hk7XVvSryPSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss functions for classification and regression tasks\n",
        "classification_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "regression_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define metrics to track during training\n",
        "classification_loss_metric = tf.keras.metrics.Mean(name='classification_loss')\n",
        "regression_loss_metric = tf.keras.metrics.Mean(name='regression_loss')\n",
        "classification_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='classification_accuracy')"
      ],
      "metadata": {
        "id": "PAIND17htbZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model checkpoint"
      ],
      "metadata": {
        "id": "Idagr9VTycf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \".ipynb_checkpoints\"\n",
        "log_file_path = \"logging.txt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer_model,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    with open(ckpt_manager.latest_checkpoint + '-epoch.txt', 'w') as f:\n",
        "        f.write(str(epoch+1))\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
        "\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(f\"Epoch {epoch + 1}: C-Loss - {classification_loss_metric.result()}, R-Loss - {regression_loss_metric.result()}, C-Acc - {classification_accuracy_metric.result()}\\n\")\n",
        "\n",
        "# Function to load the model and epoch information\n",
        "def load_checkpoint():\n",
        "    latest_checkpoint = ckpt_manager.latest_checkpoint\n",
        "    epoch = 0\n",
        "    if latest_checkpoint:\n",
        "        ckpt.restore(latest_checkpoint)\n",
        "        print('Latest checkpoint restored!!')\n",
        "        try:\n",
        "            with open(latest_checkpoint + '-epoch.txt', 'r') as f:\n",
        "                epoch = int(f.read())\n",
        "            print(f\"Model restored from {latest_checkpoint}, trained up to epoch {epoch}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Epoch information not found for {latest_checkpoint}\")\n",
        "    else:\n",
        "        print(\"No checkpoint found.\")\n",
        "    return epoch"
      ],
      "metadata": {
        "id": "pVJgMMM3tDEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_checkpoint_epoch = load_checkpoint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHcCeNx5uxkm",
        "outputId": "556075c1-98d0-45d4-e611-b3dc42bd5442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using gradient tape for getting derivatives of loss functions w.r.t. weights then applyiing to optimizer => BACKPROPAGATION"
      ],
      "metadata": {
        "id": "FvJXFgK6yq5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(input_data1, input_data2, targets_classification, targets_regression):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        classification_predictions, regression_predictions = transformer_model(input_data1, input_data2, training=True)\n",
        "\n",
        "        # Calculate losses\n",
        "        classification_loss = classification_loss_fn(targets_classification, classification_predictions)\n",
        "        regression_loss = regression_loss_fn(targets_regression, regression_predictions)\n",
        "        total_loss = classification_loss + regression_loss\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(total_loss, transformer_model.trainable_variables)\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.apply_gradients(zip(gradients, transformer_model.trainable_variables))\n",
        "\n",
        "    # Update metrics\n",
        "    classification_loss_metric.update_state(classification_loss)\n",
        "    regression_loss_metric.update_state(regression_loss)\n",
        "    classification_accuracy_metric.update_state(targets_classification, classification_predictions)"
      ],
      "metadata": {
        "id": "iTrq4aJ-n94H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "v71cVYuDyzl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy training data (replace with actual data)\n",
        "num_train_samples = 1000\n",
        "num_classes = 10\n",
        "train_data1 = np.random.rand(num_train_samples, sequence_length1).astype(np.float32)\n",
        "train_data2 = np.random.rand(num_train_samples, sequence_length2).astype(np.float32)\n",
        "train_targets_classification = np.random.randint(0, num_classes, size=(num_train_samples,), dtype=np.int32)\n",
        "train_targets_regression = np.random.rand(num_train_samples, 1).astype(np.float32)\n",
        "\n",
        "# Define hyperparameters and training configurations\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "metrics_names = ['C_loss', 'R_loss', 'C_acc']\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(latest_checkpoint_epoch,num_epochs):\n",
        "    # Reset the metrics at the start of each epoch\n",
        "    classification_loss_metric.reset_states()\n",
        "    regression_loss_metric.reset_states()\n",
        "    classification_accuracy_metric.reset_states()\n",
        "\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,num_epochs))\n",
        "    pb_i = tf.keras.utils.Progbar(train_data1.shape[0], stateful_metrics=metrics_names)\n",
        "\n",
        "    # Iterate over the training dataset in batches\n",
        "    for i in range(0, len(train_data1), batch_size):\n",
        "        batch_data1 = train_data1[i:i+batch_size]\n",
        "        batch_data2 = train_data2[i:i+batch_size]\n",
        "        batch_targets_classification = train_targets_classification[i:i+batch_size]\n",
        "        batch_targets_regression = train_targets_regression[i:i+batch_size]\n",
        "\n",
        "        # Perform a training step\n",
        "        train_step(batch_data1, batch_data2, batch_targets_classification, batch_targets_regression)\n",
        "\n",
        "        values=[('C_loss',classification_loss_metric.result()),\n",
        "                ('R_loss',regression_loss_metric.result()),\n",
        "                ('C_acc',classification_accuracy_metric.result())]\n",
        "\n",
        "        pb_i.add(batch_size, values=values)\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        save_checkpoint(epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j5GAxFmne9a",
        "outputId": "e5889fe6-495f-40b4-cbf9-e4b31744deee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/5\n",
            "1024/1000 [==============================] - 34s 33ms/step - C_loss: 3.9266 - R_loss: 0.3402 - C_acc: 0.1100\n",
            "\n",
            "epoch 2/5\n",
            "1024/1000 [==============================] - 13s 13ms/step - C_loss: 2.3790 - R_loss: 0.3402 - C_acc: 0.1120\n",
            "\n",
            "epoch 3/5\n",
            "1024/1000 [==============================] - 18s 18ms/step - C_loss: 2.3557 - R_loss: 0.3402 - C_acc: 0.0990\n",
            "\n",
            "epoch 4/5\n",
            "1024/1000 [==============================] - 17s 17ms/step - C_loss: 2.3522 - R_loss: 0.3402 - C_acc: 0.1000\n",
            "\n",
            "epoch 5/5\n",
            "1024/1000 [==============================] - 13s 13ms/step - C_loss: 2.3474 - R_loss: 0.3402 - C_acc: 0.0980\n",
            "Saving checkpoint for epoch 5 at .ipynb_checkpoints/ckpt-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "USrGav0uy3a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size and sequence lengths\n",
        "batch_size = 1\n",
        "sequence_length1 = 5\n",
        "sequence_length2 = 15\n",
        "\n",
        "# Generate random input data\n",
        "input_data1 = np.random.rand(batch_size, sequence_length1).astype(np.float32)\n",
        "input_data2 = np.random.rand(batch_size, sequence_length2).astype(np.float32)\n",
        "\n",
        "# Call the model to get predictions\n",
        "classification_predictions, regression_predictions = transformer_model(input_data1, input_data2, training=True)\n",
        "\n",
        "# Process the outputs\n",
        "# For classification predictions\n",
        "classification_predictions = classification_predictions\n",
        "class_predictions = tf.argmax(classification_predictions, axis=-1)\n",
        "\n",
        "# For regression predictions\n",
        "regression_values = tf.cast(tf.round(regression_predictions), dtype=tf.int32)\n",
        "\n",
        "\n",
        "print(\"Classification Predictions:\", class_predictions)\n",
        "print(\"Regression Predictions:\", regression_values)\n"
      ],
      "metadata": {
        "id": "HeR0yuyQRlwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e638176-abad-4e6c-fa08-9d42b9eb3db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Predictions: tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "Regression Predictions: tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}